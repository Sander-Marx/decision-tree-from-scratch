{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2f2a5b",
   "metadata": {},
   "source": [
    "# Import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ff90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb06348",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97a516b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.13</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.66</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.32</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>13.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.74</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   citric acid  residual sugar    pH  sulphates  alcohol type\n",
       "0         0.13            1.60  3.34       0.59      9.2    1\n",
       "1         0.10            2.80  3.60       0.66     10.2    1\n",
       "2         0.32            1.90  3.20       0.55      9.5    1\n",
       "3         0.29           13.65  3.00       0.60      9.5    0\n",
       "4         0.26            2.00  3.41       0.74      9.2    1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"wine_dataset.csv\", header = None, skiprows = 1)\n",
    "data.columns = [\"citric acid\"] + [\"residual sugar\"] + [\"pH\"] + [\"sulphates\"] + [\"alcohol\"] + [\"type\"] \n",
    "data.iloc[:, :-1] = data.iloc[:, :-1].astype(np.float64)\n",
    "data.iloc[:, -1] = data.iloc[:, -1].astype(str)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e037e29",
   "metadata": {},
   "source": [
    "# Node and Tree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03aa437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature = None,threshold = None, left = None, right = None, gain = None, value = None, maj_label = None):\n",
    "        # check node\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gain = gain\n",
    "        # leaf node\n",
    "        self.value = value\n",
    "        self.maj_label = maj_label  # To store labels that reach this node\n",
    "\n",
    "        \n",
    "        # printing of node\n",
    "    def print(self):\n",
    "        print(\"feature: \" + str(self.feature) + \"threshold: \" + str(self.threshold) + \"left: \" + str(self.left)\n",
    "              + \"right: \" + str(self.right) + \"gain: \" + str(self.gain))\n",
    "    \n",
    "\n",
    "class Dtree():\n",
    "    def __init__(self):\n",
    "        # the root of tree\n",
    "        self.root=None\n",
    "        \n",
    "        \n",
    "        # used for setting the root of the tree\n",
    "    def set_root(self, root):\n",
    "        self.root = root\n",
    "        \n",
    "    \n",
    "        # used to calculate the entropy\n",
    "    def entropy(self, y): # y is labels\n",
    "        # entropy formula E=−∑i=Npilog2pi\n",
    "        classes, class_count = np.unique(y, return_counts = True)\n",
    "        entropy = 0\n",
    "        for i in classes:\n",
    "            pi = len(y[y == i]) / len(y)\n",
    "            entropy -= pi * np.log2(pi)\n",
    "        return entropy\n",
    "            \n",
    "        \n",
    "        # builds a decision tree based on either the GINI index or entropy \n",
    "    def make_tree(self, dataset, is_gini):\n",
    "        X= dataset[:, :-1] \n",
    "        Y= dataset[:, -1]\n",
    "        temp_flat_data = np.ravel(X)\n",
    "        \n",
    "        # if all of the labels have the same value return that value\n",
    "        if np.all(Y == Y[0]):\n",
    "            value_leaf = Y[0]\n",
    "            return Node(value = value_leaf)\n",
    "        # else if all the data points have equal feature value return the most common label\n",
    "        elif np.all(X == temp_flat_data[0]):\n",
    "            value_leaf = self.most_common(Y)\n",
    "            return Node(value = value_leaf)\n",
    "        # else get best split based on entropy\n",
    "        else:\n",
    "            (best_feature, best_threshold, best_data_left, best_data_right, best_gain) = self.best_split(X, Y, is_gini)\n",
    "            if best_gain is None:\n",
    "                return Node(value = self.most_common(Y))\n",
    "            \n",
    "            left_subtree = self.make_tree(best_data_left, is_gini)\n",
    "            right_subtree = self.make_tree(best_data_right, is_gini)\n",
    "            majority_label = self.most_common(Y)\n",
    "            return Node(feature = best_feature, threshold = best_threshold, left = left_subtree, right = right_subtree, gain = best_gain, maj_label = majority_label)\n",
    "    \n",
    "        # finds the most common value for label or majority label\n",
    "    def most_common(self, lst):\n",
    "        y = list(lst)\n",
    "        return max(y, key = y.count)\n",
    "    \n",
    "    \n",
    "        # returns the best split based on either the GINI index or entropy impurity\n",
    "    def best_split(self, X, y, is_gini):\n",
    "        samples, features = np.shape(X)     \n",
    "        # calculate entropy/gini of parent\n",
    "        measure = self.gini if is_gini else self.entropy\n",
    "        parent_measure = measure(y)    \n",
    "        feature = None\n",
    "        threshold = None\n",
    "        gain = None\n",
    "        data_left = None\n",
    "        data_right = None\n",
    "        data_left_out = None\n",
    "        data_right_out = None\n",
    "         \n",
    "        for f in range(features):\n",
    "            mean = np.mean(X[:,f])\n",
    "            # can use np.median but is slower   \n",
    "            data_left = np.where(X[:, f] <= mean)\n",
    "            data_right = np.where(X[:, f] > mean)\n",
    "            measure_left = measure(y[data_left])\n",
    "            measure_right = measure(y[data_right]) \n",
    "            summ = (len(data_left[0]) + len(data_right[0]))\n",
    "            weighted_measure =(measure_left * (len(data_left[0]) / summ)) + (measure_right * (len(data_right[0]) / summ))\n",
    "            # if the weighted entropy/gini is smaller than the parent entropy/gini we want to split on this node    \n",
    "            if weighted_measure < parent_measure:\n",
    "                gain = parent_measure - weighted_measure\n",
    "                parent_measure = weighted_measure\n",
    "                feature = f\n",
    "                threshold = mean\n",
    "                data_left_out = np.concatenate((X[data_left], y[data_left].reshape(-1, 1)), axis = 1)\n",
    "                data_right_out = np.concatenate((X[data_right], y[data_right].reshape(-1, 1)), axis = 1)          \n",
    "        return feature, threshold, data_left_out, data_right_out,  gain\n",
    "    \n",
    "        # returns an array of predictions\n",
    "    def predict(self, y):\n",
    "        predictions = [self.make_predictions(x, self.root) for x in y]\n",
    "        return predictions\n",
    "    \n",
    "        # checks the thresholds and the feature value and makes a prediction based on them recursively\n",
    "    def make_predictions(self, x, node):\n",
    "        if node.value != None: return node.value \n",
    "        feature_v = x[node.feature]\n",
    "        if feature_v <= node.threshold:\n",
    "            return self.make_predictions(x, node.left)\n",
    "        else:\n",
    "            return self.make_predictions(x, node.right)\n",
    "        \n",
    "        \n",
    "        # prints the tree with indentations\n",
    "    def print_tree(self, node, spacing = \"\"):\n",
    "        if node.value is not None:\n",
    "            print(f\"{spacing}Predict: {node.value}\")\n",
    "            return\n",
    "\n",
    "        print(f\"{spacing}[Feature {node.feature} <= {node.threshold}] (Gain: {node.gain})\")\n",
    "    \n",
    "        print(f\"{spacing}--> Left:\")\n",
    "        self.print_tree(node.left, spacing + \"  \")\n",
    "    \n",
    "        print(f\"{spacing}--> Right:\")\n",
    "        self.print_tree(node.right, spacing + \"  \")\n",
    "        \n",
    "        \n",
    "        # used to calculate the gini\n",
    "    def gini(self, y):\n",
    "        # gini formula\n",
    "        classes, class_count = np.unique(y, return_counts = True)\n",
    "        gini = 0\n",
    "        for i in classes:\n",
    "            pi = len(y[y == i]) / len(y)\n",
    "            gini += pi ** 2\n",
    "        return 1 - gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c0f50",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ea4f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # returns a decision tree based on features X and labels y\n",
    "def learn(X, y, impurity_measure, is_prune = False, prune_data_ratio = 0.2): \n",
    "    if is_prune == True:\n",
    "        X_train_prune, X_test_prune, Y_train_prune, Y_test_prune = train_test_split(X, y, test_size = prune_data_ratio, random_state = 41)\n",
    "        data_prune = np.concatenate((X_train_prune, Y_train_prune), axis = 1)\n",
    "        tree = Dtree()\n",
    "        root = tree.make_tree(data_prune, impurity_measure == \"gini\")\n",
    "        tree.set_root(root)\n",
    "        \n",
    "        new_root = prune_tree(tree, X_test_prune,  Y_test_prune)\n",
    "        tree.set_root(new_root)        \n",
    "        return tree\n",
    "    else:\n",
    "        print(\"Entropy impurity\" if impurity_measure == \"entropy\" else \"Gini impurity\")\n",
    "        dataset = np.concatenate((X, y), axis = 1)  \n",
    "        tree = Dtree()\n",
    "        root = tree.make_tree(dataset, impurity_measure == \"gini\")\n",
    "        tree.set_root(root)\n",
    "        return tree\n",
    "        \n",
    "    # prunes the tree by replacing subtrees with leaf nodes that have the majority label\n",
    "def prune_tree(tree, X_p, Y_p, node = None):\n",
    "    if node is None:\n",
    "        node = tree.root\n",
    "    if node.value is not None:\n",
    "        return node\n",
    "\n",
    "    # combine identical values\n",
    "    if (node.left.value is not None) and (node.right.value is not None) and (node.left.value == node.right.value):\n",
    "        return Node(value=node.left.value)\n",
    "    \n",
    "    # Recursively prune the left and right children\n",
    "    if node.left is not None:\n",
    "        node.left = prune_tree(tree, X_p, Y_p, node = node.left)\n",
    "    if node.right is not None:\n",
    "        node.right = prune_tree(tree, X_p, Y_p, node = node.right)\n",
    "    # Save old node information\n",
    "    old_value = node.value\n",
    "    old_left = node.left\n",
    "    old_right = node.right\n",
    "\n",
    "    # Set node to a leaf node with majority label\n",
    "    node.value = node.maj_label\n",
    "    node.left = None\n",
    "    node.right = None\n",
    "\n",
    "    # Test the accuracy with the node converted to a leaf\n",
    "    pruned_accuracy = accuracy_score(Y_p, tree.predict(X_p))\n",
    "\n",
    "    # Revert the node back to its original state\n",
    "    node.value = old_value\n",
    "    node.left = old_left\n",
    "    node.right = old_right\n",
    "\n",
    "    # If pruning did not reduce accuracy, replace the subtree with the majority label\n",
    "    original_accuracy = accuracy_score(Y_p, tree.predict(X_p))\n",
    "    if pruned_accuracy >= original_accuracy:\n",
    "        node.value = node.maj_label\n",
    "        node.left = None\n",
    "        node.right = None\n",
    "        return node\n",
    "    return node   \n",
    "\n",
    "    \n",
    "    # prints and returns the accuracy of the decision tree\n",
    "def measure_accuracy(tree, measure, is_pruning, data_type, X, Y):\n",
    "    Y_PRED = tree.predict(X)\n",
    "    acc = accuracy_score(Y, Y_PRED)\n",
    "    print(f\"decision tree with {measure} and {'post' if is_pruning else 'no'} pruning has a {data_type} accuracy of: {acc}\")\n",
    "    return {\"measure\": measure, \"is_pruning\": is_pruning, \"accuracy\": acc}\n",
    "\n",
    "    # by providing the different settings with their accuracy measures (on the validation data) it prints and returns the best setting among them\n",
    "def get_best_setting(settings):\n",
    "    max_value = -1\n",
    "    best_setting = None\n",
    "    for setting in settings:\n",
    "        if setting[\"accuracy\"] > max_value:\n",
    "            max_value = setting[\"accuracy\"]\n",
    "            best_setting = setting\n",
    "    print('best setting is: ')\n",
    "    print(best_setting)\n",
    "    print()\n",
    "    return best_setting\n",
    "\n",
    "    # used for evaluating the best setting by measuring its accuracy on the test data and printing out the results\n",
    "def evaluate_best_setting(best_setting, gini_tree, gini_tree_pr, entropy_tree, entropy_tree_pr):\n",
    "    if best_setting[\"measure\"] == 'gini':\n",
    "        if best_setting[\"is_pruning\"] == False:\n",
    "            measure_accuracy(gini_tree, 'gini', False, 'test', X_test, Y_test)\n",
    "        else:\n",
    "            measure_accuracy(gini_tree_pr, 'gini', True, 'test', X_test, Y_test)\n",
    "    elif best_setting[\"is_pruning\"] == False:\n",
    "        measure_accuracy(entropy_tree, 'entropy', False, 'test', X_test, Y_test)\n",
    "    else:\n",
    "        measure_accuracy(entropy_tree_pr, 'entropy', True, 'test', X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c2c87",
   "metadata": {},
   "source": [
    "# 1.4 - Test the implementation with wine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ad4d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy impurity\n",
      "decision tree with entropy and no pruning has a validation accuracy of: 0.865625\n",
      "decision tree with entropy and no pruning has a training accuracy of: 1.0\n",
      "\n",
      "decision tree with entropy and post pruning has a validation accuracy of: 0.86875\n",
      "decision tree with entropy and post pruning has a training accuracy of: 0.9057857701329164\n",
      "\n",
      "Gini impurity\n",
      "decision tree with gini and no pruning has a validation accuracy of: 0.85625\n",
      "decision tree with gini and no pruning has a training accuracy of: 1.0\n",
      "\n",
      "decision tree with gini and post pruning has a validation accuracy of: 0.8625\n",
      "decision tree with gini and post pruning has a training accuracy of: 0.9089132134480062\n",
      "\n",
      "best setting is: \n",
      "{'measure': 'entropy', 'is_pruning': True, 'accuracy': 0.86875}\n",
      "\n",
      "decision tree with entropy and post pruning has a test accuracy of: 0.921875\n",
      "time needed for creating tree: 1.7682678699493408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Import data\n",
    "X = data.iloc[:, :-1]\n",
    "Y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "X = np.asarray(X, dtype = object)\n",
    "Y = np.asarray(Y, dtype = object)\n",
    "np.random.seed(41)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val_test, Y_train, Y_val_test = train_test_split(X, Y, test_size = 0.2, random_state = 2)\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X_val_test, Y_val_test, test_size = 0.5, random_state = 2)\n",
    "\n",
    "# training data: X_train, y_train\n",
    "# validation data: X_train_val, y_train_val\n",
    "# test data: X_test, Y_test\n",
    "\n",
    "# entropy without pruning\n",
    "entropy_tree = learn(X_train, Y_train, \"entropy\", False)\n",
    "entropy_val_acc = measure_accuracy(entropy_tree, 'entropy', False, 'validation', X_train_val, Y_train_val)\n",
    "measure_accuracy(entropy_tree, 'entropy', False, 'training', X_train, Y_train)\n",
    "print()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "# entropy with pruning\n",
    "entropy_tree_pr = learn(X_train, Y_train, \"entropy\", True, 0.2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "entropy_pr_val_acc = measure_accuracy(entropy_tree_pr, 'entropy', True, 'validation', X_train_val, Y_train_val)\n",
    "measure_accuracy(entropy_tree_pr, 'entropy', True, 'training', X_train, Y_train)\n",
    "print()\n",
    "\n",
    "\n",
    "# gini without pruning\n",
    "gini_tree = learn(X_train, Y_train, \"gini\", False)\n",
    "gini_val_acc = measure_accuracy(gini_tree, 'gini', False, 'validation', X_train_val, Y_train_val)\n",
    "measure_accuracy(gini_tree, 'gini', False, 'training', X_train, Y_train)\n",
    "print()\n",
    "\n",
    "\n",
    "# gini with pruning\n",
    "gini_tree_pr = learn(X_train, Y_train, \"gini\", True, 0.2)\n",
    "gini_pr_val_acc = measure_accuracy(gini_tree_pr, 'gini', True, 'validation', X_train_val, Y_train_val)\n",
    "measure_accuracy(gini_tree_pr, \"gini\", True, 'training', X_train, Y_train)\n",
    "print()\n",
    "\n",
    "\n",
    "# model selection\n",
    "# (settings are in this order because with same accuracy: pruning > no pruning, gini > entropy)\n",
    "settings = [gini_pr_val_acc, entropy_pr_val_acc, gini_val_acc, entropy_val_acc]\n",
    "best_setting = get_best_setting(settings)\n",
    "\n",
    "\n",
    "# evaluation\n",
    "evaluate_best_setting(best_setting, gini_tree, gini_tree_pr, entropy_tree, entropy_tree_pr)\n",
    "print('time needed for creating tree: ' + str(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca1880b",
   "metadata": {},
   "source": [
    "# 1.5 - Compare to other implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec303a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of sklearn DecisionTreeClassifier with impurity measure entropy on validation data:  0.903125\n",
      "accuracy of sklearn DecisionTreeClassifier with impurity measure entropy on training data:  1.0\n",
      "\n",
      "accuracy of sklearn DecisionTreeClassifier with impurity measure gini on validation data:  0.89375\n",
      "accuracy of sklearn DecisionTreeClassifier with impurity measure gini on training data:  1.0\n",
      "time needed for creating tree: 0.00797891616821289\n",
      "\n",
      "We need to compare the accuracy on test data to our implementation's test accuracy\n",
      "accuracy of sklearn DecisionTreeClassifier with impurity measure entropy on test data:  0.91875\n",
      "accuracy of sklearn DecisionTreeClassifier with impurity measure gini on test data:  0.91875\n",
      "time needed for creating tree: 0.005984067916870117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import time\n",
    "\n",
    "# create entropy tree from sklearn\n",
    "start_time = time.time()\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 2)\n",
    "classifier.fit(X_train, Y_train)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "pred_val = classifier.predict(X_train_val)\n",
    "pred_train = classifier.predict(X_train)\n",
    "pred_test = classifier.predict(X_test)\n",
    "print(\"accuracy of sklearn DecisionTreeClassifier with impurity measure entropy on validation data: \", accuracy_score(Y_train_val, pred_val))\n",
    "\n",
    "print(\"accuracy of sklearn DecisionTreeClassifier with impurity measure entropy on training data: \", accuracy_score(Y_train, pred_train))\n",
    "print()\n",
    "\n",
    "# create gini tree from sklearn\n",
    "start_time_gini = time.time()\n",
    "classifier = DecisionTreeClassifier(criterion = 'gini', random_state = 2)\n",
    "classifier.fit(X_train, Y_train)\n",
    "end_time_gini = time.time()\n",
    "elapsed_time_gini = end_time_gini - start_time_gini\n",
    "pred_val = classifier.predict(X_train_val)\n",
    "pred_train = classifier.predict(X_train)\n",
    "pred_test = classifier.predict(X_test)\n",
    "print(\"accuracy of sklearn DecisionTreeClassifier with impurity measure gini on validation data: \", accuracy_score(Y_train_val, pred_val))\n",
    "print(\"accuracy of sklearn DecisionTreeClassifier with impurity measure gini on training data: \", accuracy_score(Y_train, pred_train))\n",
    "print('time needed for creating tree: ' + str(elapsed_time))\n",
    "print()\n",
    "print(\"We need to compare the accuracy on test data to our implementation's test accuracy\")\n",
    "print(\"accuracy of sklearn DecisionTreeClassifier with impurity measure entropy on test data: \", accuracy_score(Y_test, pred_test))\n",
    "print(\"accuracy of sklearn DecisionTreeClassifier with impurity measure gini on test data: \", accuracy_score(Y_test, pred_test))\n",
    "print('time needed for creating tree: ' + str(elapsed_time_gini))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb35df8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
